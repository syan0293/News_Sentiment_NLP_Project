{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00784821-3acf-4ee5-bb63-883633f9e9db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           category                              title  \\\n",
      "0          business    UK house prices dip in November   \n",
      "1          business  LSE 'sets date for takeover deal'   \n",
      "2             sport    Harinordoquy suffers France axe   \n",
      "3          business  Barclays shares up on merger talk   \n",
      "4          politics   Campaign 'cold calls' questioned   \n",
      "...             ...                                ...   \n",
      "1552       business  Hariri killing hits Beirut shares   \n",
      "1553       politics  MPs issued with Blackberry threat   \n",
      "1554  entertainment  Bollywood DVD fraudster is jailed   \n",
      "1555          sport                Ireland v USA (Sat)   \n",
      "1556           tech  Row brewing over peer-to-peer ads   \n",
      "\n",
      "                                                content  \n",
      "0      UK house prices dipped slightly in November, ...  \n",
      "1      The London Stock Exchange (LSE) is planning t...  \n",
      "2      Number eight Imanol Harinordoquy has been dro...  \n",
      "3      Shares in UK banking group Barclays have rise...  \n",
      "4      Labour and the Conservatives are still teleph...  \n",
      "...                                                 ...  \n",
      "1552   Shares in Solidere, the Lebanese company foun...  \n",
      "1553   MPs will be thrown out of the Commons if they...  \n",
      "1554   A major distributor of pirated DVDs of Bollyw...  \n",
      "1555   Saturday 20 November  Lansdowne Road, Dublin ...  \n",
      "1556   Music download networks are proving popular n...  \n",
      "\n",
      "[1557 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "data=pd.read_csv('news-dataset.csv',sep='\\t')\n",
    "df=pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961ad8ef-72a0-4456-b20c-a7484bcde468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fed112-49a5-49ca-b2f7-d939e8e58432",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bec9d3d-337b-42f7-a112-5990e60e0b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory C:\\temp\\my_dir already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.abspath('C:/temp/my_dir')\n",
    "\n",
    "if not os.path.exists(base_dir):\n",
    "    os.makedirs(base_dir)\n",
    "    print(f\"Directory {base_dir} created successfully.\")\n",
    "else:\n",
    "    print(f\"Directory {base_dir} already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5858fb9-6bb7-45b5-b37c-dec61c7f875a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from C:\\temp\\my_dir\\mlp_text_classification\\tuner0.json\n",
      "WARNING:tensorflow:From C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 12 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,560,256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,285</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │       \u001b[38;5;34m2,560,256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │           \u001b[38;5;34m1,285\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,561,541</span> (13.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,561,541\u001b[0m (13.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,561,541</span> (13.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,561,541\u001b[0m (13.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.85      0.76      0.80        70\n",
      "entertainment       0.70      0.80      0.74        40\n",
      "     politics       0.83      0.90      0.87        61\n",
      "        sport       0.99      0.93      0.96        81\n",
      "         tech       0.79      0.82      0.80        60\n",
      "\n",
      "     accuracy                           0.85       312\n",
      "    macro avg       0.83      0.84      0.83       312\n",
      " weighted avg       0.85      0.85      0.85       312\n",
      "\n",
      "Weighted F1-score: 0.847\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import Hyperband\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import random\n",
    "\n",
    "\n",
    "# Prepare the dataset\n",
    "X = df['content']\n",
    "y = df['category']\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # maximum number of words to keep in the vocabulary\n",
    "max_len = 100  # maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Define the model builder function for hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Embedding layer with tunable dimension\n",
    "    model.add(Embedding(input_dim=max_words, \n",
    "                        output_dim=hp.Int('embedding_dim', min_value=50, max_value=200, step=50), \n",
    "                        input_length=max_len))\n",
    "    \n",
    "    # Flatten layer to feed into dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layers with tunable units and dropout rate\n",
    "    for i in range(hp.Int('num_dense_layers', 1, 3)):\n",
    "        model.add(Dense(units=hp.Int(f'dense_units_{i}', min_value=64, max_value=256, step=64), activation='relu'))\n",
    "        model.add(Dropout(rate=hp.Float(f'dropout_rate_{i}', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    # Output layer with softmax activation\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "    # Compile the model with a tunable learning rate\n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Set up the tuner\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory=base_dir,\n",
    "    project_name='mlp_text_classification'\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(X_train_padded, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best model and evaluate it\n",
    "best_model_nn = tuner.get_best_models(num_models=1)[0]\n",
    "print(best_model_nn.summary())\n",
    "best_model_nn.save('best_modelnn.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model_nn.predict(X_test_padded)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Calculate and print classification report with weighted F1-score\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(f\"Weighted F1-score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afe3fb-d812-45ef-bc19-ad1827fa8ec0",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77371bfd-abc0-41d1-911f-454e7cf88e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from C:\\temp\\my_dir\\cnn_text_classification\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 16 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">94</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">134,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m2,000,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m94\u001b[0m, \u001b[38;5;34m96\u001b[0m)              │         \u001b[38;5;34m134,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m3,104\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m165\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,137,765</span> (8.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,137,765\u001b[0m (8.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,137,765</span> (8.15 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,137,765\u001b[0m (8.15 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.82      0.97      0.89        70\n",
      "entertainment       0.87      0.85      0.86        40\n",
      "     politics       0.98      0.77      0.86        61\n",
      "        sport       0.99      0.95      0.97        81\n",
      "         tech       0.83      0.88      0.85        60\n",
      "\n",
      "     accuracy                           0.89       312\n",
      "    macro avg       0.90      0.89      0.89       312\n",
      " weighted avg       0.90      0.89      0.89       312\n",
      "\n",
      "Weighted F1-score: 0.894\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import Hyperband\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Prepare the dataset\n",
    "X = df['content']\n",
    "y = df['category']\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # maximum number of words to keep in the vocabulary\n",
    "max_len = 100  # maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Define the model builder function for hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, \n",
    "                        output_dim=hp.Int('embedding_dim', min_value=50, max_value=200, step=50), \n",
    "                        input_length=max_len))\n",
    "    \n",
    "    model.add(Conv1D(filters=hp.Int('filters', min_value=32, max_value=256, step=32), \n",
    "                     kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), \n",
    "                     activation='relu'))\n",
    "    \n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Set up the tuner\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory=base_dir,\n",
    "    project_name='cnn_text_classification'\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(X_train_padded, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best model and evaluate it\n",
    "best_model_cnn = tuner.get_best_models(num_models=1)[0]\n",
    "best_model_cnn.summary()\n",
    "best_model_cnn.save('best_modelcnn.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model_cnn.predict(X_test_padded)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Calculate and print classification report with weighted F1-score\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(f\"Weighted F1-score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266375ed-a095-4822-bff4-19bac482b981",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab37b60b-9fc0-43db-a10f-837aacbd5c9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from C:\\temp\\my_dir\\lstm_text_classification\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "C:\\Users\\柒雅\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">67,840</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m200\u001b[0m)            │       \u001b[38;5;34m2,000,000\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m67,840\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │           \u001b[38;5;34m8,320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m645\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,076,805</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,076,805\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,076,805</span> (7.92 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,076,805\u001b[0m (7.92 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.84      0.90      0.87        70\n",
      "entertainment       0.88      0.95      0.92        40\n",
      "     politics       0.90      0.85      0.87        61\n",
      "        sport       0.99      0.98      0.98        81\n",
      "         tech       0.86      0.80      0.83        60\n",
      "\n",
      "     accuracy                           0.90       312\n",
      "    macro avg       0.89      0.90      0.89       312\n",
      " weighted avg       0.90      0.90      0.90       312\n",
      "\n",
      "Weighted F1-score: 0.897\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras_tuner import Hyperband\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Prepare the dataset\n",
    "X = df['content']\n",
    "y = df['category']\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 10000  # maximum number of words to keep in the vocabulary\n",
    "max_len = 100  # maximum sequence length\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# Define the model builder function for hyperparameter tuning\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=max_words, \n",
    "                        output_dim=hp.Int('embedding_dim', min_value=50, max_value=200, step=50), \n",
    "                        input_length=max_len))\n",
    "    \n",
    "    model.add(LSTM(units=hp.Int('lstm_units', min_value=64, max_value=256, step=64), return_sequences=False))\n",
    "    model.add(Dropout(rate=hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=128, step=32), activation='relu'))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])), \n",
    "                  loss='sparse_categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Set up the tuner\n",
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=10,\n",
    "    factor=3,\n",
    "    directory= base_dir,\n",
    "    project_name='lstm_text_classification'\n",
    ")\n",
    "\n",
    "# Run the hyperparameter search\n",
    "tuner.search(X_train_padded, y_train, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Get the best model and evaluate it\n",
    "best_model_lstm = tuner.get_best_models(num_models=1)[0]\n",
    "best_model_lstm.summary()\n",
    "best_model_lstm.save('best_modellstm.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = best_model_lstm.predict(X_test_padded)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "\n",
    "# Calculate and print classification report with weighted F1-score\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "print(f\"Weighted F1-score: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b52d8fa-d538-42fc-90e2-1dd51bc9a6d0",
   "metadata": {},
   "source": [
    "# Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae469dad-4b3f-455c-b153-215cc169dd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgin Radio offers 3G broadcast</td>\n",
       "      <td>UK broadcaster Virgin Radio says it will beco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Police chief backs drinking move</td>\n",
       "      <td>A chief constable has backed the introduction...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Labour seeks to quell feud talk</td>\n",
       "      <td>Labour's leadership put on a show of unity at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Edwards tips Idowu for Euro gold</td>\n",
       "      <td>World outdoor triple jump record holder and B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ivanovic seals Canberra victory</td>\n",
       "      <td>Serbia's Ana Ivanovic captured her first WTA ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                             title  \\\n",
       "0   0  Virgin Radio offers 3G broadcast   \n",
       "1   1  Police chief backs drinking move   \n",
       "2   2   Labour seeks to quell feud talk   \n",
       "3   3  Edwards tips Idowu for Euro gold   \n",
       "4   4   Ivanovic seals Canberra victory   \n",
       "\n",
       "                                             content  \n",
       "0   UK broadcaster Virgin Radio says it will beco...  \n",
       "1   A chief constable has backed the introduction...  \n",
       "2   Labour's leadership put on a show of unity at...  \n",
       "3   World outdoor triple jump record holder and B...  \n",
       "4   Serbia's Ana Ivanovic captured her first WTA ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "best_model_lstm = load_model('best_modellstm.h5')\n",
    "\n",
    "new_data = pd.read_csv('news-challenge.csv',sep='\\t')\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fee7c261-cd75-4b9d-b52a-ec89f93945af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "max_len = 100  # same as training\n",
    "max_words = 10000  \n",
    "labels = ['business', 'entertainment', 'politics', 'sport', 'tech']\n",
    "\n",
    "new_data_seq = tokenizer.texts_to_sequences(new_data['content'])\n",
    "\n",
    "new_data_padded = pad_sequences(new_data_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "predictions = best_model_lstm.predict(new_data_padded)\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "predicted_labels = [labels[i] for i in predicted_classes]\n",
    "\n",
    "new_data['category'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b29d7cb-e3e4-4966-a8c3-89f482c0c4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Virgin Radio offers 3G broadcast</td>\n",
       "      <td>UK broadcaster Virgin Radio says it will beco...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Police chief backs drinking move</td>\n",
       "      <td>A chief constable has backed the introduction...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Labour seeks to quell feud talk</td>\n",
       "      <td>Labour's leadership put on a show of unity at...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Edwards tips Idowu for Euro gold</td>\n",
       "      <td>World outdoor triple jump record holder and B...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Ivanovic seals Canberra victory</td>\n",
       "      <td>Serbia's Ana Ivanovic captured her first WTA ...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                             title  \\\n",
       "0   0  Virgin Radio offers 3G broadcast   \n",
       "1   1  Police chief backs drinking move   \n",
       "2   2   Labour seeks to quell feud talk   \n",
       "3   3  Edwards tips Idowu for Euro gold   \n",
       "4   4   Ivanovic seals Canberra victory   \n",
       "\n",
       "                                             content  category  \n",
       "0   UK broadcaster Virgin Radio says it will beco...      tech  \n",
       "1   A chief constable has backed the introduction...  politics  \n",
       "2   Labour's leadership put on a show of unity at...  politics  \n",
       "3   World outdoor triple jump record holder and B...     sport  \n",
       "4   Serbia's Ana Ivanovic captured her first WTA ...     sport  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1597e121-7990-4321-924e-74337dc7199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  category\n",
      "0   0      tech\n",
      "1   1  politics\n",
      "2   2  politics\n",
      "3   3     sport\n",
      "4   4     sport\n"
     ]
    }
   ],
   "source": [
    "competiton_data = new_data.drop(columns=['content','title'])\n",
    "print(competiton_data.head())\n",
    "competiton_data.to_csv('Group05QBUS6850_2024S2Com.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a8a269-9f49-421a-b888-a596b0345610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
